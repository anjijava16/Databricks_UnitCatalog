{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c314f2d-055a-4747-9988-6011a2d69814",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Elasticsearch\n",
    "\n",
    "<img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt5ebe80fb665aef6b/5ea8c8f26b62d4563b6ecec2/brand-elasticsearch-220x130.svg\" width=\"300\">\n",
    "\n",
    "1. Launch a cluster in your workspace or choose an existing cluster.\n",
    "2. Once the new cluster is running, go to the \"Libraries\" tab of that cluster, and click \"Install new\" -> choose \"Maven\" -> enter the maven coordinates `org.elasticsearch:elasticsearch-spark-30_2.12:8.4.3` -> click \"Install\". If running into errors like `org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version` while the ES connection is verified, consider install newer versions that matches your ES service.\n",
    "3. Once the installation has finished, attach this notebook to the cluster, and run write and/or read operations against your Elasticsearch cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0f18d4-4b89-414a-9a82-60c6d00a3a16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Important**: In the following cells, replace `<ip-address>`, `<port>`, `<ssl>`, `<hostname>` and `<index>` with your Elasticsearch configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b29ea68-c5f4-4a02-bec8-3301dabf90bb",
     "showTitle": true,
     "title": "Test connectivity to your Elasticsearch cluster"
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "nc -vz ip-address port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db7c0f1-11be-4238-b4c0-95c2eec8590d",
     "showTitle": true,
     "title": "Make trivial test dataframe"
    }
   },
   "outputs": [],
   "source": [
    "people = spark.createDataFrame( [ (\"Bilbo\",     50), \n",
    "                                  (\"Gandalf\", 1000), \n",
    "                                  (\"Thorin\",   195),  \n",
    "                                  (\"Balin\",    178), \n",
    "                                  (\"Kili\",      77),\n",
    "                                  (\"Dwalin\",   169), \n",
    "                                  (\"Oin\",      167), \n",
    "                                  (\"Gloin\",    158), \n",
    "                                  (\"Fili\",      82), \n",
    "                                  (\"Bombur\",  None)\n",
    "                                ], \n",
    "                                [\"name\", \"age\"] \n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90ec043a-4350-459b-8640-260bf9309dca",
     "showTitle": true,
     "title": "Write to Elasticsearch"
    }
   },
   "outputs": [],
   "source": [
    "# Overwrite the data each time\n",
    "\n",
    "# NOTE: We **must** set the es.nodes.wan.only property to 'true' so that the connector will connect to the node(s) specified by the `es.nodes` parameter.\n",
    "#       Without this setting, the ES connector will try to discover ES nodes on the network using a broadcast ping, which won't work.\n",
    "#       We want to connect to the node(s) specified in `es.nodes`.\n",
    "( people.write\n",
    "  .format( \"org.elasticsearch.spark.sql\" )\n",
    "  .option( \"es.nodes\",   hostname )\n",
    "  .option( \"es.port\",    port     )\n",
    "  .option( \"es.net.ssl\", ssl      )\n",
    "  .option( \"es.nodes.wan.only\", \"true\" )\n",
    "  .mode( \"overwrite\" )\n",
    "  .save( f\"{index}\" )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cdba138-252d-41fa-a0a6-b5eea8d5994f",
     "showTitle": true,
     "title": "Read from Elasticsearch"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: We **must** set the es.nodes.wan.only property to 'true' so that the connector will connect to the node(s) specified by the `es.nodes` parameter.\n",
    "#       Without this setting, the ES connector will try to discover ES nodes on the network using a broadcast ping, which won't work.\n",
    "#       We want to connect to the node(s) specified in `es.nodes`.\n",
    "df = (spark.read\n",
    "      .format( \"org.elasticsearch.spark.sql\" )\n",
    "      .option( \"es.nodes\",   hostname )\n",
    "      .option( \"es.port\",    port     )\n",
    "      .option( \"es.net.ssl\", ssl      )\n",
    "      .option( \"es.nodes.wan.only\", \"true\" )\n",
    "      .load( f\"{index}\" )\n",
    "     )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6673d86d-204f-427c-8840-6e90d91d37f8",
     "showTitle": true,
     "title": "Write to Delta"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a Delta table called table_name\n",
    "df.write.format(\"delta\").saveAsTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Import from Elasticsearch - 2023-09-29",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
